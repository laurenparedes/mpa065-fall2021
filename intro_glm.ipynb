{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Generalized Linear Models in R</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 1. Generalized Linear Models\n",
    "\n",
    "  - Family of models _including_ linear models\n",
    "  - Generalization of linear models to included categorical and/or non-normaly distributed dependent/outcome variable\n",
    "  - Examples:\n",
    "    + Binomial logistic regression\n",
    "    + Multinomial regression\n",
    "    + Poisson regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ## 1.1 Note on Terminology\n",
    "  - General Linear Model $\\ne$ General**ized** Linear Model\n",
    "\n",
    "  - General linear model refers to models with a continuous outcome variable, and assumption of normality\n",
    "    + ANOVA (and friends)\n",
    "    + Linear regression\n",
    "\n",
    "  - Term General**ized** Linear Model used to refer to a family of models for categorical and/or non-normally distributed outcome variables \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2 Binomial Logistic Regression\n",
    "\n",
    "  - Linear regression assumes a continuous outcome variable\n",
    "\n",
    "  - If the outcome variable is _not_ continuous, we need a different approach. \n",
    "        \n",
    "  - In the case of a binary outcome variable, we model $\\text{Pr}(y_i = 1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.1 Logistic Regression vs. Linear Regression\n",
    "    \n",
    "Differences from linear regression:\n",
    "  - Assumes outcome is bounded by $0$ and $1$\n",
    "    + that is $0 \\le E(y_i) = \\pi_i \\le 1$\n",
    "  - Variance of $y$ is _not_ constant (i.e., not the same for all $y_i$)\n",
    "  - Similarly, variance of $\\varepsilon$ is not constant\n",
    "  - Computational differences (i.e., closed-form vs numerical methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.2 Components of Generalized Linear Models\n",
    "Recall the form of the linear model:\n",
    "        \n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p + \\varepsilon $$ \n",
    "        \n",
    "\n",
    "which can also be written in matrix notation as\n",
    "        \n",
    "$$\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} $$\n",
    "        \n",
    "where $\\mathbf{X} \\boldsymbol{\\beta}$ is the systemic component and $\\boldsymbol{\\varepsilon}$ is the random component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.1 Components of Generalized Linear Model (cont.)\n",
    "\n",
    "Form of GLM:\n",
    "$$g(\\mu) = \\mathbf{X} \\boldsymbol{\\beta}$$\n",
    "\n",
    "Generalized linear models have 3 components:\n",
    "1. Systemic component\n",
    "  - Same as linear regression (e.g., $\\mathbf{X} \\boldsymbol{\\beta}$)\n",
    "2. Response distribution assumption\n",
    "  - Random component of the model\n",
    "  - Specifies the probabilistic mechanism by which responses were generated\n",
    "3. Link function\n",
    "  - This is $g(\\cdot)$ in equation above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.2 The Link Function\n",
    "\n",
    "Link function is a characteristic feature of generalized linear models\n",
    "\n",
    "A link function:\n",
    "1. Connects the systemic component to response (i.e., \"links\" them) \n",
    " - Allows us to map a linear function with range $\\left( - \\infty, \\infty \\right)$ to some new range; e.g., $\\left(0, 1\\right)$\n",
    "2. Differs according to the species of GLM in question (and even within)\n",
    "3. Similar to \"activation functions\" in artificial neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.3 Binomial Logistic Regression\n",
    "Logistic regression with a single predictor:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\pi(x_1) & = \\frac{\\exp{(\\beta_0 + \\beta_1 x_1 )}}{1 + \\exp{(\\beta_0 + \\beta_1 x_1 )}} \\\\\n",
    "& \\\\\n",
    "& = \\frac{1}{1 + \\exp{(- \\eta )}}\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "where $\\eta = \\beta_0 + \\beta_1 x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.4 Binomial Logistic Regression\n",
    "$$ \\pi(x_1) = \\frac{\\exp{(\\beta_0 + \\beta_1 x_1 )}}{1 + \\exp{(\\beta_0 + \\beta_1 x_1 )}} $$\n",
    "\n",
    "\n",
    "Note that the $ \\beta_0 + \\beta_1 x_1 $ in the above equation is the same as we saw in linear regression. This is called the _linear predictor_ in logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.5 Interpreting Parameter Estimates\n",
    "Interpretation of logistic regression parameter estimates:\n",
    "\n",
    "\n",
    "1. Slightly different than linear regression\n",
    "2. Recall our model is $\\text{Pr}\\left(y_i = 1\\right) = \\text{logit}^{-1}\\left(\\mathbf{X} \\boldsymbol{\\beta}\\right)$\n",
    "3. Regression parameters estimates are on logit scale (log odds), \n",
    "  - It's common to exponentiate $\\widehat{\\boldsymbol{\\beta}}$\n",
    "  - Value of $\\text{exp}\\left(\\beta_j\\right)$ is the odds ratio of 1-unit increase on $x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.6 Model Evaluation\n",
    "\n",
    "1. Recall that $R^2$ in linear regression gives us a nice method of evaluating models (i.e., proportion of variance explained). \n",
    "2. However, in logistic regression, there is no direct analogue to $R^2$ (but there are some similar measure)\n",
    "3. Thus, we tend to rely on the information-based criteria discussed previously (e.g., AIC, BIC)\n",
    "  - These also have the advantage of penalizing unnecessary model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.7 Choice of Link Function\n",
    "Several link function options for modeling binomial data:\n",
    "\n",
    "1. Logit link (most common, by far)\n",
    "2. CDF of normal distribution (probit regression)\n",
    "3. CDF of $t$-distribution (_robit_ model; robust binomial regression)\n",
    "  - Degrees of freedom parameter allows for flexibility in accommodating outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2.8 When to use Logistic Regression\n",
    "\n",
    "  - Used when outcome variable takes one of two values (e.g., $0$ or $1$, \"lived\" or \"died\") \n",
    "  - Similar structure as linear regression \n",
    "\n",
    "    + Estimate effects of predictors on outcome\n",
    "    + Can have one or many predictors\n",
    "\n",
    "  - Can answer similar kinds of questions as linear regression, for example:\n",
    "    + _What is the effect of the predictor, $x$, on the outcome $y$?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.3 Single-Variable Logistic Regression in R\n",
    "\n",
    "  - Suppose we are interested in the survival of passengers on the Titanic. \n",
    "  - In particular, suppose we want to know whether a passenger's gender impacted whether or not they survived.\n",
    "  - We can investigagte this using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "library(broom)\n",
    "\n",
    "titanic_df <- read.csv(\"data/titanic_subset.csv\")\n",
    "\n",
    "head(titanic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3.1 Fitting the Model\n",
    "- Generalized linear models are fitted using `glm()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fm1 <- glm(survived ~ sex, titanic_df, family = binomial(link = \"logit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tidy(fm1)           # show table of regression parameter estimates\n",
    "\n",
    "glance(fm1)         # show fit indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3.2 Using Fitted Model to Make Predictions\n",
    "\n",
    "- We can use fitted model object (e.g., `fm1`) to make predictions for new data\n",
    "- Use the `predict()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict(fm1, newdata = data.frame(sex = \"male\"), type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Why do we Need Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fm1b <- lm(survived ~ sex, titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tidy(fm1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.1 Simulated Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n <- 1500                           # set sample size\n",
    "beta0 <- 1.2                        # specify regression parameters\n",
    "beta1 <- -0.7\n",
    "\n",
    "x <- rnorm(n)           \n",
    "\n",
    "invlogit <- function(eta) {         # Define a function to compute the inverse logit. \n",
    "    res <- 1/(1 + exp(-eta))        # Recall this is our link function.\n",
    "    return(res)\n",
    "}\n",
    "\n",
    "pr  <- invlogit(beta0 + beta1*x)\n",
    "y   <- rbinom(n, 1, pr)\n",
    "dat <- data.frame(y, x)\n",
    "\n",
    "lm1 <- lm(y ~ x, dat)               # Fit a linear model and a binomial logistic model\n",
    "\n",
    "glm1 <- glm(y ~ x, dat, family = binomial(link = \"logit\"))\n",
    "\n",
    "tidy(lm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1.1 Comparing our Linear Model and Bionomial Logistic Model\n",
    "\n",
    " - Compare linear regression to logistic \n",
    " - Logistic does better job of recovering betas (i.e., `1.2` and `-0.7`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tidy(lm1)\n",
    "\n",
    "tidy(glm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# 4. Multivariate Binomial Logistic Regression\n",
    "\n",
    "Suppose now we want to investigate the effect of both sex and age on survival. We use the model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fm2 <- glm(survived ~ sex + age, titanic_df, family = binomial(link = \"logit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tidy(fm2)\n",
    "\n",
    "glance(fm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict(fm2, newdata = data.frame(sex = \"female\", age = 40), type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Poisson Regression Models in R</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Poisson Regression\n",
    "\n",
    "1. Form of Poisson model for single predictor\n",
    "\n",
    "$$ \\text{log}(\\mu) = \\beta_0 + \\beta_1 x_1 $$ \n",
    "\n",
    "2. Link function is $\\text{log}(\\cdot)$\n",
    "\n",
    "3. We use Poisson regression when we model count data   % (e.g., $y_i \\in \\{0, 1, 2, \\dots \\}$)\n",
    "  - Number of offspring an individual has\n",
    "  -  Number bacterial colonies in Petri dish\n",
    "4. As we saw with logistic regression, we _could_ use a linear model instead (Don't do this). but our parameter estimates would be biased, and our model inaccurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.1 Poisson Distribution\n",
    "<center>\n",
    "<img src=images/poisson_dist.png width = \"860\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.2 Poisson Regression\n",
    "\n",
    "1. As with linear and logistic regression, we can use Poisson regression to estimate effects of predictors on some outcome\n",
    "\n",
    "2. We can also use fitted Poisson regression models to predict future values of some outcome variable given known values for the covariates\n",
    "\n",
    "3. Frequently used for modeling rare events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2.1 Assumptions of Poisson Regression\n",
    "1. Log-transformed outcomes are linearly related to predictors \n",
    "2. Observations are independent \n",
    "3. Distributional assumption:  $y_i | x_i \\sim \\text{Poisson}(\\lambda_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2.2 Assumptions of Poisson Regression (cont.)\n",
    "\n",
    " - Note that the assumption $y_i | x_i \\sim \\text{Poisson}(\\lambda_i)$ has some important implications.\n",
    "\n",
    " - The Poisson distribution has a single parameter, $\\lambda$, which is both its mean and variance. \n",
    "\n",
    " - It is frequently the case we will have data where the variance greatly exceeds the mean. When this happens, it is wise to consider similar alternatives to the Poisson model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2.3 Similar Alternatives to Poisson Models\n",
    "1. Quasi-Poisson regression\n",
    "2. Zero-inflated Poisson regression\n",
    "3. Negative Binomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2.4 Evaluation of Poisson Regression Models\n",
    "\n",
    " - As with logistic regression, there is no direct counterpart to the $R^2$ in linear regression\n",
    "\n",
    " - Poisson regression models can be compared using AIC and BIC as we saw with linear and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.2.5 Interpreting Poisson Regression Parameters\n",
    " - We can exponentiate Poisson regression parameter estimates, and then treat them multiplicative effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Poisson Model for Number of Procedures\n",
    "\n",
    "Suppose we want to model the number of procedure for diabetes patients admitted to the hospital. We use several Poisson models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dia_df <- read.csv(\"data/diabetes_data_clean.csv\")\n",
    "\n",
    "\n",
    "dim(dia_df)        # dimensions of dataframe (i.e., number of rows and columns)\n",
    "names(dia_df)      # get column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.1 Checking Mean and Variance\n",
    "\n",
    "- Recall the assumptions of Poisson models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean(dia_df$num_procedures)\n",
    "var(dia_df$num_procedures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.2 Fitting Poisson Regression Model\n",
    "\n",
    "- As with binomial logistic regression, we use `glm()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fm5 <- glm(num_procedures ~ number_diagnoses, dia_df, family = poisson(link = \"log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tidy(fm5)\n",
    "\n",
    "glance(fm5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.2.1 Plotting our Model's Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_dia_subset <- sample_frac(dia_df, 0.3)\n",
    "\n",
    "ggplot(df_dia_subset, aes(x = number_diagnoses, y = num_procedures)) +\n",
    "    geom_jitter(alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.3 Adding Predictor Variables\n",
    "\n",
    "- Like linear and logistic regression, we can add arbitrary number of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fm6 <- glm(num_procedures ~ number_diagnoses + num_medications + time_in_hospital, \n",
    "           dia_df, \n",
    "           family = poisson(link = \"log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tidy(fm6)\n",
    "glance(fm6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.3.1 Plotting our Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df_dia_subset, aes(x = time_in_hospital, y = num_procedures, colour = number_diagnoses)) +\n",
    "    geom_jitter(alpha = 0.8) + \n",
    "    geom_smooth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>GLMs with Interaction Effects in R</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 7. Interaction Effects\n",
    "\n",
    "- Often called \"moderation\", or statistical moderation effects\n",
    "- One variable \"behaves\" differently depending on the level another\n",
    "- E.g. cigarette smoking is an _increasingly strong_ predictor of lung cancer as age increases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 8. Logistic Regression with Interaction Effect\n",
    "\n",
    "- Using `titanic` data \n",
    "- Predicting survival using age and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "titanic_df <- read.csv(\"data/titanic_subset.csv\")\n",
    "\n",
    "head(titanic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 8.1 Fitting Model with Interaction Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fm3 <- glm(survived ~ sex + age + age*sex, titanic_df, family = binomial(link = \"logit\"))\n",
    "\n",
    "tidy(fm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 8.1.1 Plotting the Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(titanic_df, aes(x = age, y = as.numeric(survived), color = sex)) +\n",
    "    stat_smooth(method = \"glm\", alpha = 0.2, size = 2, aes(fill = sex)) +\n",
    "    geom_point(position=position_jitter(height = 0.03, width = 0)) +\n",
    "    xlab(\"Age\") + \n",
    "    ylab(\"Pr(survived)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 8.1.2 Predictions using Fitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fm2 <- glm(survived ~ age + sex + age*sex, titanic_df, family = binomial(link = \"logit\"))\n",
    "\n",
    "predict(fm2, newdata=data.frame(age = 50, sex = \"female\"), type = \"response\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
